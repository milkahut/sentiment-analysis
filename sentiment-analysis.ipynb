{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with RNN\n",
    "\n",
    "In this notebook, \n",
    "I build a model that implement a recurrent neural network with PyTorch to perform sentiment analysis on movie reviews.\n",
    "The dataset is taken from  IMDB  reviews. The reviews are accompanied by labels of the sentiment: positive or negative. \n",
    "To build a model for sentiment analysis, actually we can use a simple feedforward network. However with such a framework, the model will only consider individual words to predict the sentiment. With RNN, the prediction will be more accurate because we can also include information about the sequence of the words.  So the model will not only consider the individual words, but also  the order they appear in.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "I consider a model with the following architecture. First I pass in the words from the review to an embedding layer. Then the new embeddings will be passed to LSTM cells. They will add recurrent connections to the network and give us the ability to include information about the sequence of words in the movie review. Finally, I pass the LSTM outputs to a sigmoid output layer. I use a sigmoid function because a sigmoid will output predicted, sentiment values between 0-1. \n",
    " \n",
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2000])\n",
    "print()\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "First I will make the data into the proper form. I clean it up a bit by converting the text into lowercase and get rid of periods and punctuation.  \n",
    "\n",
    "Moreover since the reviews are delimited with newline characters `\\n`, I can split the text into individual reviews with the delimiter `\\n`. \n",
    "\n",
    "I  then combined them back together and split again to collect all individual  words that are used in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    " \n",
    "reviews = reviews.lower()  \n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split reviews\n",
    "reviews_split = all_text.split('\\n')\n",
    "\n",
    "# collect words used in the reviews\n",
    "all_text = ' '.join(reviews_split)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integers Encoding\n",
    "\n",
    "The model will use embedding layers that require us to pass in integers to the model. So we need to encode each word in the vocabulary with an integer. In the following I make a dictionary that maps words to integers. I then\n",
    "convert the reviews to integers and store them in a new list called `reviews_ints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key = counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)}\n",
    "\n",
    "## tokenize each review and  store them in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the dictionary, I print out  the content of the first tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized review: \n",
      " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
     ]
    }
   ],
   "source": [
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also convert the labels to integers 0 and 1 and store the new encoded labels in the list `encoded_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = labels.split('\\n') \n",
    "encoded_labels = np.array([1 if c == 'positive' else 0 for c in all_labels] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers \n",
    "\n",
    "To make sure that our reviews are in a good shape for standard processing, I observe whether there are some outliers. \n",
    "In the following, I check if the data  contains  extremely long or short reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews with 0 length: 1\n",
      "The length of the longest reviews: 2514\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Number of reviews with 0 length: {}\".format(review_lens[0]))\n",
    "print(\"The length of the longest reviews: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since there is an empty review, I simply remove it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty reviews\n",
    "index = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "reviews_ints = [reviews_ints[ii] for ii in index]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Padding and Truncating \n",
    "Note that the   length of the longest review  is way too big for the model. To handle this,\n",
    "I  truncate super long reviews. More precisely, in dealing with both short and very long reviews, I shape  the reviews into a specific length.  I define such a length with `seq_length`.  For reviews shorter than   `seq_length`, I pad with 0s on its beginning,\n",
    "e.g. if the review is `['best', 'movie', 'ever']`,or  `[117, 18, 128]` as integers, I pad the review so it will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. For reviews longer than `seq_length`, I simply truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200. \n",
    "\n",
    "The following is\n",
    "a function that returns an array `features` that contains the padded data  of a standard size. I will then pass the array to the model. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad reviews by adding 0s in the beginning so the length of each review is equal to seq_length\n",
    "def pad_features(reviews_ints, seq_length): \n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    " \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length] \n",
    "    \n",
    "    return features\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in the nice shape. I split it into training, validation, and test sets.\n",
    "The fraction of data that I keep for the training set is 80% and the remaining is split in half to create the data for\n",
    "validation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \t\t(20000, 200) \n",
      "Val set: \t\t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data  \n",
    "n = int(split_frac * len(features))\n",
    "train_x, rem_x = features[:n], features[n:]\n",
    "train_y, rem_y = encoded_labels[:n], encoded_labels[n:]\n",
    "\n",
    "ns = int(0.5 * len(rem_x))\n",
    "test_x, val_x = rem_x[:ns], rem_x[ns:]\n",
    "test_y, val_y = rem_y[:ns], rem_y[ns:]\n",
    "\n",
    "## print out the shapes of the data\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nVal set: \\t\\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "I create DataLoaders for this data. First by using [TensorDataset](https://pytorch.org/docs/stable/data.html#),\n",
    " I create a known format for accessing our data. Note that the TensorDataset takes in an input set of data and a target set of data with the same first dimension and then creates a dataset.\n",
    " I then create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffle the data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model with PyTorch\n",
    "\n",
    "\n",
    "I consider a model which basically  consists of an \n",
    " [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size, an [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers, a\n",
    "a fully-connected output layer that maps the LSTM layer outputs to a desired output_size, and \n",
    "a sigmoid activation layer which turns all outputs into a value 0-1. Note that it will return only the last sigmoid output as the output of this model.\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "Note that we need an embedding layer because there are more than 74000 words in the review vocabulary. It is simply not efficient to one-hot encode that many classes. So, instead of one-hot encoding, I use an embedding layer and use thelayer as a lookup table.   It's ok to just make a new layer, since we will use it only for dimensionality reduction and let the model learn the weights.\n",
    "\n",
    "\n",
    "### The LSTM Layers\n",
    "\n",
    "I create an LSTM  to use in the model. It will take  in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        # setting up the layers\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first= True)        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # return last sigmoid output and hidden state\n",
    "        embeds = self.embedding(x)\n",
    "        r_output, hidden = self.lstm(embeds, hidden)\n",
    "        r_output = r_output.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(r_output)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        sig_out = sig_out.view(batch_size,-1)\n",
    "        sig_out = sig_out[:,-1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size): \n",
    "        # create two new tensors: n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "         \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instatiate the model \n",
    "\n",
    "Before training the model, I need to instantiate it. I  define the hyperparameters as follows.\n",
    "First, I define\n",
    "`vocab_size` which is the size of the vocabulary, `output_size` which is the size of the desired output, i.e. the number of class scores we want to output: positive or negative,\n",
    " `embedding_dim` which is the number of columns in the embedding lookup table which is the size of our embeddings, \n",
    " `hidden_dim` which is the number of units in the hidden layers of our LSTM cells, and \n",
    " `n_layers` which is the number of LSTM layers in the network. \n",
    " \n",
    " Moreover I also need to define the learning rate, and the loss, optimization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 200\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now the model is ready for the training. To make the computation faster, I use GPU if it is available.\n",
    " \n",
    "\n",
    "I will use a new kind of cross entropy loss called Binary Cross Entropy Loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss). It applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "In this training I consider 4 epochs.I iterate through the training dataset four times. Moreover, to\n",
    "prevent exploding gradients I set the clip to 5. So the maximum gradient value to clip at is five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.667548... Val Loss: 0.669033\n",
      "Epoch: 1/4... Step: 200... Loss: 0.774982... Val Loss: 0.738713\n",
      "Epoch: 1/4... Step: 300... Loss: 0.525721... Val Loss: 0.540313\n",
      "Epoch: 1/4... Step: 400... Loss: 0.456010... Val Loss: 0.538258\n",
      "Epoch: 2/4... Step: 500... Loss: 0.431597... Val Loss: 0.505338\n",
      "Epoch: 2/4... Step: 600... Loss: 0.261225... Val Loss: 0.466534\n",
      "Epoch: 2/4... Step: 700... Loss: 0.433863... Val Loss: 0.462178\n",
      "Epoch: 2/4... Step: 800... Loss: 0.289795... Val Loss: 0.443104\n",
      "Epoch: 3/4... Step: 900... Loss: 0.175254... Val Loss: 0.482436\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.384781... Val Loss: 0.459099\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.192630... Val Loss: 0.446066\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.419111... Val Loss: 0.532251\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.244968... Val Loss: 0.535243\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.254958... Val Loss: 0.445546\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.259188... Val Loss: 0.528085\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.099251... Val Loss: 0.484037\n"
     ]
    }
   ],
   "source": [
    "# First I check if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "# training params\n",
    "epochs = 4 # \n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU  if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # creating new variables for the hidden state, otherwise\n",
    "        # I would backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs or LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # create new variables for the hidden state, otherwise\n",
    "                # I would backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Now I want to test my model. I want to see how my model, after training, performs on the test_data. I check the average loss and the accuracy over the test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.504\n",
      "Test accuracy: 0.800\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "So the model have 80% accuracy. Now I want to input just one example review at a time  without a label, and see what sentiment the model will predict. Will it predict correctly or not.\n",
    "\n",
    "I make a `predict` function that takes in a trained model, a plain text_review,  a sequence length, and prints whether a positive or negative review is detected! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):  \n",
    "    test_review = test_review.lower()\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "    test_words = test_text.split()\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "    \n",
    "    features = pad_features(test_ints, sequence_length)\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    h = net.init_hidden(batch_size)\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    net.eval()\n",
    "    output, h = net(feature_tensor, h)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    if pred.item() == 1 :\n",
    "        print('POSITIVE review is detected!')\n",
    "    else:\n",
    "        print('NEGATIVE review is detected!')\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE review is detected!\n"
     ]
    }
   ],
   "source": [
    "# call function \n",
    "seq_length=200\n",
    "predict(net, test_review_neg, seq_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
